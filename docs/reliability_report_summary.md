# Heterogeneous Legal Dataset Reliability Report - Executive Summary

## Overview

**Final Reliability Score: 0.693** (on a scale of 0 = poor, 1 = excellent)

This report evaluates a heterogeneous legal dataset containing multiple data sources (CSV, JSON, XML, XLSX, JSONL, TXT) with intentional schema variations, value inconsistencies, and structural differences to test data fusion capabilities.

---

## Metric Group Summary

| Metric Group | Score | Weight | Weighted Contribution | Status | Key Issue |
|-------------|-------|--------|----------------------|--------|-----------|
| **Schema** | 0.717 | 20% | 0.143 | Good | Low schema overlap (15.2%) |
| **Value** | 0.644 | 20% | 0.129 | Moderate | 1,140 numeric normalization errors |
| **Structural** | 0.495 | 15% | 0.074 | **Poor** | Complete structural divergence (score: 1.0) |
| **Entity Resolution** | 0.876 | 15% | 0.131 | Excellent | Strong duplicate detection |
| **OCR** | 0.730 | 5% | 0.037 | Good | Minimal corruption |
| **Semantic** | 0.714 | 25% | 0.179 | Good | 57% clause misclassification rate |

---

## Detailed Metric Analysis

### 1. Schema Metrics (Score: 0.717)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Schema Divergence Score** | 1.0000 | 1.000 | Maximum divergence detected | `client_id` (CSV) vs `custFullNm` (JSON) vs `cid` (XML) |
| **Schema Overlap Percentage** | 15.2174% | 0.152 | Only 15% of fields align across sources | Client name appears as `company_name`, `custFullNm`, `clientName` |
| **Field-Type Variation Rate** | 0.0000 | 1.000 | No type mismatches | All date fields are strings (consistent inconsistency) |

**Analysis**: High schema fragmentation (1.0) but low overlap (15.2%) indicates different naming conventions across sources. Field types are consistently strings, avoiding type conversion errors.

---

### 2. Value Metrics (Score: 0.644)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Value Inconsistency Rate** | 0.0617 | 0.938 | 6.17% of values conflict | Same client: "Stonebridge Motors Inc" vs "Stonebridge Motors Company" |
| **Numeric Normalization Errors** | 1,140 | 0.000 | **Critical Issue** | `$12,000,000` vs `12000000` vs `"$1,675.00"` |
| **Date Format Diversity** | 2.0000 | 0.750 | 2 distinct formats | `2022-09-25` vs `15/09/22` vs `25-Oct-2021` vs Unix timestamps |
| **Phone Format Variations** | 2.0000 | 0.889 | 2 format types | `(555) 125-5506` vs `+1-555-338-9279` vs `555.489.2584` |

**Analysis**: Numeric normalization is the primary concern (1,140 errors). Currency values appear with/without symbols, commas, and quotes. Date formats vary significantly but are manageable.

---

### 3. Structural Metrics (Score: 0.495)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Structural Depth Variance** | 0.2500 | 0.990 | Low variance in nesting | JSON has nested `financials` object, CSV is flat |
| **Structural Distance Score** | 1.0000 | 0.000 | **Maximum divergence** | Flat CSV vs nested JSON vs XML hierarchy |

**Analysis**: Complete structural mismatch (1.0) between flat (CSV) and hierarchical (JSON/XML) formats. This requires significant transformation logic.

---

### 4. Semantic Metrics (Score: 0.714)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Clause Embedding Misclassification** | 0.5714 | 0.429 | 57% of clauses misclassified | "Reviewed contract" vs "analyzed the underlying agreement documents" |
| **LLM Semantic Equivalence Accuracy** | 1.0000 | 1.000 | Perfect detection | LLM correctly identifies semantically equivalent clauses |

**Analysis**: High misclassification rate (57%) indicates embedding models struggle with semantic equivalence. However, LLM-based detection achieves 100% accuracy, suggesting hybrid approach is needed.

---

### 5. Entity Resolution Metrics (Score: 0.876)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Duplicate Detection Rate** | 0.0164 | 0.984 | 1.64% duplicates found | Same matter: `MAT-1011` vs `MTR-1010` |
| **Conflict Severity Index** | 0.3248 | 0.675 | Moderate conflicts | Same client with different IDs: `CL-1001` vs `CL-1002` |
| **Golden Record Completion** | 0.9683 | 0.968 | 96.8% complete after fusion | Most entities successfully merged |

**Analysis**: Strong entity resolution performance. Low duplicate rate (1.64%) and high completion (96.8%) indicate effective matching despite ID inconsistencies.

---

### 6. OCR Metrics (Score: 0.730)

| Metric | Raw Value | Normalized | Interpretation | Example |
|--------|-----------|------------|----------------|---------|
| **Corrupted Token Ratio** | 0.0000 | 1.000 | No corruption detected | All text files clean |
| **Unexpected Character Frequency** | 0.0270 | 0.460 | 2.7% unexpected chars | Special characters from PDF extraction |

**Analysis**: Minimal OCR issues. Text quality is high, suitable for downstream NLP processing.

---

## Dataset Composition

The dataset contains:

| Data Type | Format | Count | Purpose |
|-----------|--------|-------|---------|
| **Client Records** | CSV, JSON, XML, XLSX | 4 files | Schema variation testing |
| **Matter Records** | CSV, JSON | 2 files | Entity resolution testing |
| **Billing Entries** | CSV | 1 file | Value normalization testing |
| **Documents** | TXT (PDF/DOCX extracted) | 40 files | Semantic analysis testing |
| **Filings** | TXT | 20 files | Document type classification |
| **Regulations** | TXT | 6 files | Reference data |
| **Emails** | JSONL | 1 file | Unstructured data testing |
| **Billing Narratives** | TXT | 15 files | Natural language processing |

**Total**: 89 files across 6 formats

---

## Key Findings

### Strengths
1. **Entity Resolution**: 0.876 score - excellent duplicate detection and golden record completion
2. **Schema Consistency**: No type mismatches despite naming variations
3. **OCR Quality**: Clean text extraction with minimal corruption
4. **LLM Semantic Detection**: 100% accuracy in identifying equivalent clauses

### Critical Issues
1. **Numeric Normalization**: 1,140 errors - currency and numeric values require extensive parsing
2. **Structural Divergence**: Complete mismatch (1.0) between flat and hierarchical formats
3. **Schema Overlap**: Only 15.2% field alignment across sources
4. **Clause Classification**: 57% misclassification rate using embeddings alone

### Recommendations
1. **Priority 1**: Implement robust numeric/currency normalization pipeline
2. **Priority 2**: Develop structural transformation layer for format conversion
3. **Priority 3**: Use hybrid approach (embeddings + LLM) for semantic matching
4. **Priority 4**: Create canonical schema mapping for 15% overlap fields

---

## Calculation Notes

- **Final Score**: Weighted sum of all metric groups = 0.693
- **Normalization**: All metrics normalized to 0-1 scale (higher = better)
- **Direction**: "higher_is_better" means higher raw values indicate better reliability
- **Weighted Contribution**: Group score Ã— Weight = Contribution to final score

---

## Data Examples

### Schema Variation Example
- **CSV**: `client_id`, `company_name`, `annual_revenue`
- **JSON**: `id`, `custFullNm`, `financials.turnover`
- **XML**: `cid`, `clientName`, `revenue`

### Value Inconsistency Example
- Same entity: "Stonebridge Motors Inc" (structured) vs "Stonebridge Motors Company" (narrative)
- Same amount: `$12,000,000` vs `12000000` vs `"$1,675.00"`

### Structural Divergence Example
- **CSV**: Flat structure `client_id,company_name,industry`
- **JSON**: Nested structure `{id, custFullNm, financials: {turnover, currency}}`

### Semantic Equivalence Example
- "Reviewed contract" (structured) = "analyzed the underlying agreement documents" (narrative)
- LLM detects equivalence; embeddings may not

---

## Mismatch Analysis Summary

**Total Mismatched Concepts/Terms: 193**

### Breakdown by Category

| Category | Count | Details |
|----------|-------|---------|
| **Schema Field Name Variations** | 23 | Different field names for same concepts across sources |
| **Value Format Variations** | 159 | Date, phone, and currency/numeric format inconsistencies |
| **Name Representation Variations** | 8 | Same entities with different name representations |
| **ID Variations** | 3 | Same entities with different identifier formats |

### Detailed Mismatch Counts

#### Schema Mismatches (23 variations)
- **Client Identifier**: 4 variations (`client_id`, `id`, `cid`, `client`)
- **Client Name**: 3 variations (`company_name`, `custFullNm`, `nm`)
- **Industry**: 3 variations (`industry`, `sector`, `cat`)
- **Revenue**: 3 variations (`annual_revenue`, `annual_turnover`, `financials.turnover`)
- **Phone**: 2 variations (`contact_phone`, `phone`)
- **Date Fields**: 5 variations (`created_at`, `created`, `entry_date`, `startDate`, `meta.registered_on`)
- **Matter Identifier**: 3 variations (`matter_id`, `file_no`, `file_id`)

#### Value Format Mismatches (159 variations)
- **Date Formats**: 106 unique formats (e.g., `2022-09-25`, `15/09/22`, `25-Oct-2021`, Unix timestamps)
- **Phone Formats**: 13 unique formats (e.g., `(555) 125-5506`, `+1-555-338-9279`, `555.489.2584`)
- **Currency/Numeric Formats**: 40 unique formats (e.g., `$12,000,000`, `12000000`, `"$1,675.00"`)

#### Name Variations (8 variations)
- CL-1001: "Stonebridge Motors Inc" vs "STONEBRIDGE MOTORS INC" vs "Stonebridge Motors Company"
- CL-1002: "Aquila Pharmaceuticals" vs "AQUILA PHARMACEUTICALS"
- CL-1005: "Nimbus Software Solutions" vs "Nimbus Soft"

#### ID Mismatches (3 variations)
- Matter IDs: `MAT-1011` vs `MTR-1010`, `MAT-1022` vs `MTR-1022`, `MAT-1032` vs `MTR-1032`

**Note**: These mismatches are intentional design features of the heterogeneous dataset to test schema matching, entity resolution, and data fusion capabilities.

---

*Report generated from analysis of heterogeneous legal dataset pack*
*Date: Analysis of final_analysis_report.md*

